# Import necessary packages
import Pkg
Pkg.add("Optim")
Pkg.add("HTTP")
Pkg.add("GLM")
Pkg.add("DataFrames")
Pkg.add("CSV")
Pkg.add("FreqTables")

using Optim
using HTTP
using GLM
using LinearAlgebra
using Random
using Statistics
using DataFrames
using CSV
using FreqTables

# Define the main function that wraps all previous steps
function estimate_models()

    #######################################################################  
    ##                   Question 1: Optimization                        ##
    #######################################################################

    println("\n---------------- Question 1: Optimization ----------------")

    # Function to maximize
    f(x) = -x[1]^4 - 10x[1]^3 - 2x[1]^2 - 3x[1] - 2

    # Negative of the function for minimization
    negf(x) = x[1]^4 + 10x[1]^3 + 2x[1]^2 + 3x[1] + 2

    # Starting value
    startval = rand(1)

    # Optimizer
    result = optimize(negf, startval, LBFGS())

    # Print the result
    println("Optimal solution (argmax): ", result.minimizer)
    println("Maximum value: ", -result.minimum)

    #######################################################################  
    ##                   Question 2: OLS Regression                      ##
    #######################################################################

    println("\n---------------- Question 2: OLS Regression ----------------")

    # Load the dataset
    url = "https://raw.githubusercontent.com/OU-PhD-Econometrics/fall-2022/master/ProblemSets/PS1-julia-intro/nlsw88.csv"
    df = CSV.read(HTTP.get(url).body, DataFrame)

    # Prepare the design matrix X and response vector y
    X = [ones(size(df,1),1) df.age df.race .== 1 df.collgrad .== 1]
    y = df.married .== 1

    # OLS objective function to minimize sum of squared residuals
    function ols(beta, X, y)
        residual = y .- X * beta  # X * beta is valid since X is a matrix
        ssr = residual' * residual  # Sum of squared residuals
        return ssr
    end

    # Initial guess for the coefficients (beta)
    initial_guess = rand(size(X, 2))

    # Use Optim to estimate OLS coefficients
    beta_hat_ols = optimize(b -> ols(b, X, y), initial_guess, LBFGS(), Optim.Options(g_tol = 1e-6, iterations = 100_000, show_trace = true))

    # Print the estimated coefficients
    println("Estimated coefficients (OLS with Optim): ", beta_hat_ols.minimizer)

    # Manual OLS calculation using matrix inversion
    beta_ols_manual = inv(X' * X) * X' * y
    println("OLS coefficients (manual matrix inversion): ", beta_ols_manual)

    # OLS estimation using the GLM package
    df.white = df.race .== 1
    bols_lm = lm(@formula(married ~ age + white + collgrad), df)
    println("OLS coefficients (GLM package): ", coef(bols_lm))

    #######################################################################  
    ##                   Question 3: Logit Model                         ##
    #######################################################################

    println("\n---------------- Question 3: Logit Model ----------------")

    # Sigmoid function
    function sigmoid(x)
        return 1.0 / (1.0 + exp(-x))
    end

    # Negative log-likelihood for the logistic regression model
    function neg_log_likelihood(beta, X, y)
        N = length(y)
        log_likelihood = 0.0
        for i in 1:N
            p = sigmoid(dot(X[i, :], beta))  # Use dot product here
            log_likelihood += y[i] * log(p) + (1 - y[i]) * log(1 - p)
        end
        return -log_likelihood  # Return the negative log-likelihood
    end

    # Initial guess for beta in the logit model
    initial_guess = zeros(size(X, 2))

    # Use Optim to estimate the logit model parameters by minimizing the negative log-likelihood
    result = optimize(b -> neg_log_likelihood(b, X, y), initial_guess, LBFGS(), Optim.Options(g_tol = 1e-6, iterations = 100_000, show_trace = true))

    # Print the estimated parameters
    println("Estimated parameters (Logit with Optim): ", result.minimizer)

    #######################################################################  
    ##                   Question 4: Logistic Regression using GLM       ##
    #######################################################################

    println("\n---------------- Question 4: Logistic Regression using GLM ----------------")

    # Estimate the logistic regression model using GLM
    logit_model_glm = glm(@formula(married ~ age + race + collgrad), df, Binomial(), LogitLink())

    # Print the estimated coefficients from the GLM package
    println("Logit coefficients (GLM package): ", coef(logit_model_glm))

    # Print comparison to the coefficients from Optim (previous result)
    println("Logit coefficients (Optim): ", result.minimizer)

    #######################################################################  
    ##                   Question 5: Multinomial Logit Model             ##
    #######################################################################

    println("\n---------------- Question 5: Multinomial Logit Model ----------------")

    # Clean the data by removing missing values for occupation
    df = dropmissing(df, :occupation)

    # Recode some occupation categories with small counts
    df[df.occupation .== 8, :occupation] .= 7
    df[df.occupation .== 9, :occupation] .= 7
    df[df.occupation .== 10, :occupation] .= 7
    df[df.occupation .== 11, :occupation] .= 7
    df[df.occupation .== 12, :occupation] .= 7
    df[df.occupation .== 13, :occupation] .= 7

    # Redefine the design matrix X and response variable y
    X = [ones(size(df, 1), 1) df.age df.race .== 1 df.collgrad .== 1]
    y = df.occupation

    # Multinomial logit: K * (C-1) parameters
    K = size(X, 2)  # Number of covariates
    C = 7           # Number of categories (7 occupations)
    initial_guess = zeros(K * (C - 1))  # Starting value for parameters

    # Softmax function (logistic link for the multinomial logit)
    function softmax(x)
        exp_x = exp.(x .- maximum(x))  # Subtract max to prevent overflow
        return exp_x / sum(exp_x)
    end

    # Negative log-likelihood function for multinomial logit
    function neg_log_likelihood_multinomial(beta, X, y, C)
        N = length(y)
        log_likelihood = 0.0

        beta_matrix = reshape(beta, K, C-1)  # Reshape beta into K x (C-1) matrix

        for i in 1:N
            logits = X[i, :]' * beta_matrix  # Transpose X[i, :] to align dimensions with beta_matrix
            logits = vcat(reshape(logits, C-1, 1), [0.0])  # Reshape logits and append base category
            probs = softmax(logits)

            # Update log-likelihood
            chosen_category = y[i]
            log_likelihood += log(probs[chosen_category])
        end

        return -log_likelihood  # Return negative log-likelihood
    end

    # Optimization using the Optim package
    result_multinom = optimize(
        b -> neg_log_likelihood_multinomial(b, X, y, C),
        initial_guess,
        LBFGS(),
        Optim.Options(g_tol = 1e-5, iterations = 100_000, show_trace = true)
    )

    # Print the estimated coefficients
    println("Estimated coefficients (Multinomial Logit with Optim): ", result_multinom.minimizer)

end  # End of main function

# Call the function at the bottom
estimate_models()


#######################################################################  
    ##                   Question 7: test           ##
    #######################################################################

    println("\n---------------- Question 7: test ----------------")

    using Test

@testset "Optimization and Regression Tests" begin

    # Test for logistic sigmoid function
    @testset "Sigmoid Function" begin
        @test sigmoid(0) ≈ 0.5
        @test sigmoid(100) ≈ 1.0 atol=1e-6
        @test sigmoid(-100) ≈ 0.0 atol=1e-6
    end

    # Test for negative log-likelihood function
    @testset "Negative Log-Likelihood (Logistic Regression)" begin
        X = [1.0 0.5; 1.0 -0.5]
        y = [1.0, 0.0]
        β = [0.0, 0.0]
        @test neg_log_likelihood(β, X, y) ≈ 1.3862943611198906  # Expected NLL for this setup
    end

    # Test for basic OLS regression via Optim
    @testset "OLS Regression" begin
        X = [1.0 0.5; 1.0 -0.5; 1.0 1.0]
        y = [1.0, 0.0, 1.0]
        β_init = [0.0, 0.0]
        result = optimize(b -> ols(b, X, y), β_init, LBFGS(), Optim.Options(g_tol=1e-6, iterations=100_000))
        @test result.converged == true
        @test length(result.minimizer) == 2
        @test isfinite(result.minimum)
    end

    # Test for the manual matrix inversion for OLS
    @testset "Manual OLS Calculation" begin
        X = [1.0 0.5; 1.0 -0.5; 1.0 1.0]
        y = [1.0, 0.0, 1.0]
        β_manual = inv(X' * X) * X' * y
        @test length(β_manual) == 2
        @test all(isfinite.(β_manual))
    end

    # Test for GLM logistic regression
    @testset "GLM Logistic Regression" begin
        df = DataFrame(married = [1, 0, 1], age = [22, 33, 44], race = [1, 0, 1], collgrad = [1, 0, 1])
        logit_model_glm = glm(@formula(married ~ age + race + collgrad), df, Binomial(), LogitLink())
        @test length(coef(logit_model_glm)) == 4
        @test all(isfinite.(coef(logit_model_glm)))
    end

    # Test for multinomial logit negative log-likelihood
    @testset "Multinomial Logit Negative Log-Likelihood" begin
        X = [1.0 0.5; 1.0 -0.5; 1.0 1.0]
        y = [1, 2, 1]
        C = 3  # Assuming three categories
        β_init = zeros(6)  # 2 covariates and 2 coefficients per category (C-1)
        nll_value = neg_log_likelihood_multinomial(β_init, X, y, C)
        @test isfinite(nll_value)
        @test nll_value < 0  # Negative log-likelihood should be negative
    end

    # Test for softmax function
    @testset "Softmax Function" begin
        x = [1.0, 2.0, 3.0]
        softmax_probs = softmax(x)
        @test sum(softmax_probs) ≈ 1.0 atol=1e-6  # Softmax should sum to 1
        @test all(softmax_probs .>= 0)  # All softmax probabilities should be non-negative
    end

    # Test for optimization of multinomial logit
    @testset "Multinomial Logit Optimization" begin
        X = [1.0 0.5; 1.0 -0.5; 1.0 1.0]
        y = [1, 2, 1]
        C = 3  # Assuming three categories
        initial_guess = zeros(6)  # 2 covariates and 2 coefficients per category (C-1)
        result_multinom = optimize(b -> neg_log_likelihood_multinomial(b, X, y, C), initial_guess, LBFGS(), Optim.Options(g_tol=1e-5, iterations=100_000))
        @test result_multinom.converged == true
        @test length(result_multinom.minimizer) == 6
        @test isfinite(result_multinom.minimum)
    end

end

